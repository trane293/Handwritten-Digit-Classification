%=========================================================================%
%         Code to train a Neural Network
%=========================================================================%
%   Author      - Anmol Sharma (DAVIET Jalandhar)
%   Supervisor  - Dr. Jaysree Chakraborty
%   Description - This code uses a Feedforward backpropogation type
%                 Pattern Recognition Neural Network. This code
%                 implements k fold cross validation for a given set of 
%                 features and target values. Output is the mean accuracy 
%                 of the classifier, mean False Positive Value, mean False
%                 Negative Value and mean EER value and is 
%                 reported in the end. 
%   License     - Closed. Only for internal use.  
%   Last Edit   - January 20th, 2015 19:38
%=========================================================================%
close all;
clc
clearvars -except invariant_values_shape invariant_values_margin labels
%==========================================================================
num = size(invariant_values_shape, 1);
num_folds = 10; % For LOOCV, put num_folds = num - 1;
%==========================================================================

featuresUsed = 'Shape';
descriptorUsed = [invariant_values_shape(:, 1:5)]; %#ok<NBRAK>
inputs = descriptorUsed';
targets = labels';

% Create a Pattern Recognition Network

groups = targets; 
cvFolds = crossvalind('Kfold', num, num_folds);   % get indices of 10-fold CV
cp = classperf(groups); % Create empty class performance tracker cp

for i = 1:num_folds       
    clear net; % Clear neural network after each fold. [CHECK]
    hiddenLayerSize = 70;
    net = patternnet(hiddenLayerSize);
    
    % Default script generated by MATLAB, so no problem.
    % Choose Input and Output Pre/Post-Processing Functions
    % For a list of all processing functions type: help nnprocess
    net.inputs{1}.processFcns = {'removeconstantrows','mapminmax'};
    net.outputs{2}.processFcns = {'removeconstantrows','mapminmax'};
    
    % Get training and Testing Indices from cvFolds
    % Puts 1 in front of index
    testIdx = (cvFolds == i);                
    trainIdx = (cvFolds ~= i);
    
    % Gets the actual index number. This will be need for input to
    % divideParam attributes.
    trInd=find(trainIdx); 
    tstInd=find(testIdx);
    
    % Training rules and other settings. 
    net.trainFcn = 'trainlm';  % Levenberg-Marquardt
    net.trainParam.epochs = 100; % Number of iterations
    net.divideFcn = 'divideind'; % Data is divided by indices
    net.divideParam.trainInd = trInd; 
    net.divideParam.testInd = tstInd;
    net.trainParam.min_grad=1e-5;% Minimum gradient value to achieve

    % Choose a Performance Function
    % For a list of all performance functions type: help nnperformance
    net.performFcn = 'mse';  % Mean squared error

    % Choose Plot Functions
    % For a list of all plot functions type: help nnplot
    
    % Train the Network
    [net,tr] = train(net,inputs,targets);

    % test using test instances

    outputs = net(inputs); % Gets the values outputted by the network
    errors = gsubtract(targets,outputs); % Computes errors 
    performance = perform(net,targets,outputs); % Returns performance between [-1 1]
    
    % Again compute training, testing indices
    trainTargets = targets .* tr.trainMask{1};
    valTargets = targets  .* tr.valMask{1};
    testTargets = targets  .* tr.testMask{1};
    
%      % Show TESTING Performance
%      figure, plotconfusion(testTargets,outputs), title(strcat('Testing Performance for Fold ' , num2str(i)));

      [c(1,i),cm{1,i}] = confusion(testTargets,outputs);
      
    % For calculating EER value and DET curve for testing data
    clear testOutputs posTestOutputs negTestOutputs
    testOutputs(1,:) = outputs(1, tr.testInd);
    testOutputs(2,:) = outputs(2, tr.testInd);  
    posTestOutputs = testOutputs(1,:);
    negTestOutputs = testOutputs(2,:);

     
    % Calculate FAR, FRR, EER.
    [EER(1,i), confInterEER(1,i), OP, confInterOP] = EER_DET_conf(posTestOutputs, negTestOutputs, 20, 10000);
    
    % The cm matrix is differently ordered than the plotconfusion matrix.
    % See Wikipedia of Confusion matrix to see what I mean to say. 
    FAR(1,i) = 100 * cm{1,i}(2,1)/(cm{1,i}(2,1) + cm{1,i}(2,2));
    FRR(1,i) = 100 * cm{1,i}(1,2)/(cm{1,i}(1,2) + cm{1,i}(1,1));
    t = strcat('FOLD ', num2str(i));
    
    % Display results for this fold
	
    fprintf('-----------------------%s---------------------\n', t);
    fprintf('Percentage Correct Classification     : %0.2f%%\n', 100*(1-c(1,i)));
    fprintf('EER Value                             : %0.2f%%\n', EER(1,i));
    fprintf('False Acceptance Rate (FAR)           : %0.2f%%\n', FAR(1,i));
    fprintf('False Rejection Rate (FRR)            : %0.2f%%\n', FRR(1,i));
    disp(   '-------------------------------------------------');
    figure, plotconfusion(testTargets,outputs);
%[tpr,fpr,thresholds] = roc(targets,outputs);
 %[c(1,i),cm,ind,per{i}] = confusion(testTargets,outputs);
 %[X{i},Y{i},T{i},AUC{i}] = perfcurve(labels(:,2),outputs(2,:),1);
end
    t = num_folds;
	fprintf('=============Code by Anmol Sharma===============\n');
    fprintf('================================================\n');
    fprintf('-----RESULTS AFTER %d FOLD CROSS VALIDATION------\n', t);
    fprintf('Percentage Correct Classification     : %0.2f%%\n', 100*(1-mean(c)));
    fprintf('EER Value                             : %0.2f%%\n', mean(EER));
    fprintf('False Acceptance Rate (FAR)           : %0.2f%%\n', mean(FAR));
    fprintf('False Rejection Rate (FRR)            : %0.2f%%\n', mean(FRR));
    disp(   '-------------------------------------------------');
    fprintf('================================================\n');

    